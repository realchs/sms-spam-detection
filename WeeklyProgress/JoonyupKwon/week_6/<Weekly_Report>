/*
GPT-1(Generative Pre Training of a language model)

language model의 예시 -> 구글검색창에 youtube deep을 입력했을떄 그 다음에 나올 단어를 예측하여 추천
language model의 특징은 특별히 레이블링이 필요하지 않음
다른 모델들은 대부분 레이블링이 필요로 했음, 사람이 직접 레이블링 하는경우에는 굉장히 오래걸리고 100% 정확한 답을 가지고 있지 않음

정말로 엄청 많은 텍스트를 가지고 언어모델을 학습시키면 오류율도 적어지고 또한 자연어의 우리가 알지못하는 특징도 학습하여서 아주 뛰어난 자연어처리모델을 만들어낼수있다는게
ㄴ GPT-1의 핵심

머신러닝을 두가지로 나눈다면
Generative model와 discriminative model으로 나눌 수 있다.
언어모델은 전자이고 타이타닉의 인적정보를 사용해서 살아남을 수 있을지 없을지 예상하는 모델이 후자이다.

discriminative의 단점은 한정된 모델에 오버피팅되기 쉽다.
generative model은 데이터가 많을수록 학습효과가 뛰어나다. 물론 데이터의 양만큼 학습시간은 더 길어진다.

GPT는 단순한 언어모델이 아니다
-문장유사도, 질의응답, 자연어 추론, 분류에도 뛰어난 성능을 보여줌

GPT는 엄청나게 많은 text를 통해서 자연어처리가 뛰어난 모델을 만드는것이다.
GPT는 뛰어나기 때문에 별도로 모델을 붙일필요가없음

GPT는 트랜스포머 디코더를 메인 디자인으로 삼았다

트랜스포머는 Attention is all you need라는 논문에서 소개된 딥러닝 모델이다.
기존 모델들이 RNN에 의존했던것과 달리 트랜스포머는 RNN에 의존하지않고, attention 메카니즘을 사용한다.

<트랜스포머>
English -> Encoder -> Decoder -> German

문장의 단어가 여러개 존재해도 한번에 연산이 가능하다
RNN은 단어가 일곱개면 일곱개를 순차적으로 계산했어야만 했다.























*/
