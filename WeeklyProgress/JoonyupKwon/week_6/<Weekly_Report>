/*
GPT-1(Generative Pre Training of a language model) - 2018 June, OpenAI

#Pre Training?
선행학습, 사전훈련, 전처리과정이라고도 하는데 Multi Layered Perceptron(MLP)에서 Weight와 Bias를 잘 초기화 시키는 방법
이 방법을 통해서 효과적으로 Layer를 쌓아 여러개의 Hidden Layer도 효율적으로 훈련 가능
또한 이는, 비지도학습이 가능하기 때문에 레이블이 되지 않은 큰 데이터를 넣어 훈련시킬 수 있는 장점이 존재하며
Drop-out, Mini-batch 방식을 사용하여 이를 생략하기도 한다.

#Fine Tuning?
기존에 학습되어져 있는 모델을 기반으로 아키텍쳐를 새로운 목적(나의 이미지 데이터에 맞게) 변형하고, 이미 학습된 모델 
Weights로부터 학습을 업데이트하는 방법을 말한다.
- 모델의 파라미터를 미세하게 조정하는 행위 (특히 딥러닝에서는 이미 존재하는 모델에 추가 데이터를 투입하여 파라미터를 업데이트)
- 파인튜닝은 정교한 파라미터 튜닝이라고 생각하면 되는데 "정교한" 과 "파라미터"가 키포인트들이다.
- 파인튜닝을 했다고 말하려면 기존에 학습이 된 레이어에 내 데이터를 추가로 학습시켜 파라미터를 업데이트 해야 한다.

*/
/*

language model의 예시 -> 구글검색창에 youtube deep을 입력했을떄 그 다음에 나올 단어를 예측하여 추천
language model의 특징은 특별히 레이블링이 필요하지 않음
다른 모델들은 대부분 레이블링이 필요로 했음, 사람이 직접 레이블링 하는경우에는 굉장히 오래걸리고 100% 정확한 답을 가지고 있지 않음

정말로 엄청 많은 텍스트를 가지고 언어모델을 학습시키면 오류율도 적어지고 또한 자연어의 우리가 알지못하는 특징도 학습하여서
아주 뛰어난 자연어처리모델을 만들어낼수있다는것이 GPT-1의 핵심

머신러닝을 두가지로 나눈다면
Generative model와 discriminative model으로 나눌 수 있다.
언어모델은 전자이고 타이타닉의 인적정보를 사용해서 살아남을 수 있을지 없을지 예상하는 모델이 후자이다.

discriminative의 단점은 한정된 모델에 오버피팅되기 쉽다.
generative model은 데이터가 많을수록 학습효과가 뛰어나다. 물론 데이터의 양만큼 학습시간은 더 길어진다.

GPT는 단순한 언어모델이 아니다
-문장유사도, 질의응답, 자연어 추론, 분류에도 뛰어난 성능을 보여줌

GPT는 엄청나게 많은 text를 통해서 자연어처리가 뛰어난 모델을 만드는것이다.
GPT는 뛰어나기 때문에 별도로 모델을 붙일필요가없음

GPT는 트랜스포머 디코더를 메인 디자인으로 삼았다

트랜스포머는 Attention is all you need라는 논문에서 소개된 딥러닝 모델이다.
기존 모델들이 RNN에 의존했던것과 달리 트랜스포머는 RNN에 의존하지않고, attention 메카니즘을 사용한다.

<트랜스포머> - 2017 June Google - Manchine Translation
English -> Encoder -> Decoder -> German
                                                                           p0 <start token>, p1 ....
                                                                           ↓
I  am  a  little  boy -> [Attention] -> [FC] -> [e0, e1, e2, e3, e4] -> [Attention] -> [FC] -> Linear -> Softmax -> 난 ... 
p0 p1  p2   p3    p4


<트랜스포머 이전 트랜드, RNN>
English => Encoder -> Decoder -> Korean
순차입력

(start token)   (end token)
↓               ↓
난     꼬마    입니다
↑       ↑       ↑
R → ↓   R → ↓   R
↑   ↓   ↑   ↓   ↑
--------------------------------------- ↑ Decoder, ↓ Encoder
[     Attention     ]    ←
↑   ↑   ↑    ↑      ↑    ↑
e0  e1 e2    e3     e4   ↑
↑   ↑   ↑    ↑      ↑    ↑
R → R → R →  R   →  R  → e4 
↑   ↑   ↑    ↑      ↑
i  am   a  littel  boy

문장의 단어가 여러개 존재해도 한번에 연산이 가능하다
RNN은 단어가 일곱개면 일곱개를 순차적으로 계산했어야만 했다.

트랜스포머 인코더를 사용하는 BERT
트랜스포머 디코더를 사용하는 GPT

GPT-1
Natural Language Inference - 2문장이 주어졌을때 두문장의 관계(ex 참거짓), (Sentence1 Sentence2 Label)
Question Answering - 딥러닝 모델에게 관련된 정보와, 질문을 주면 정보에서 정답을 찾는지 테스트, (Sentence1 Sentence2, Label)
Semantic Similarity - 두 문장이 비슷한지 아닌지, (Sentence1, Sentence2, Label)
Classification - 주어진 문장을 분류 (Sentence1, Label)

Train - 총 2번의 학습이 있습니다.
방대한 양의 label이 없는 텍스트라를 통한 언어모델 비지도학습 후, 파인튜닝(지도학습)

GPT-1은 트랜스포머의 디코더로 구성되어 있다
기존에는 목적에따라 레이어를 추가로 배치해야했는데, 이는 적지않은 시간과 노력이 필요했다
GPT는 이를 제외시켰다. 단순히 모델 그대로 유지하고 모델에 입력값을 넣고 최적화하는것이 파인튜닝의 전부이다.

I am Happy -> Pretrained LM(Transformer Decoder) -> Linear -> Softmax -> positive
Sentence1 $(delimiter) Sentence 2 -> Pretrained LM(Transformer Decoder) -> Linear -> Softmax -> entailment

<Trend before GPT 1>

Well trained model -> Task Specific model -> prediction
                               ↑ 
       Introduce significant amount of task specific customization

<GPT-1>
GPT-1 -> prediction
  ↑
just use structured input for task specific tuning so pre-trained LM(Transformer Decoder) can process


#Tokenization Evolution (BPE: Byte Pair Encoding)

Word 임베딩의 장점은 단어간의 유사도를 가지고있음, 단점은 학습시에 보지 못했던 단어들은 단순히 유사도가 전혀 존재할수없는
제로백터로 처리되기 때문에 신조어, 오타자에 상당히 약하다.

Char 임베딩을 사용할 경우에 단어는 일정한 Char의 집합이기 때문에 단어간의 유사도가 확실히 word 임베딩에 비해서 떨어짐

...












*/
